% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/auto_catboost_reg.R
\name{auto_catboost_reg}
\alias{auto_catboost_reg}
\title{Automatic model tununig using Bayesian optimazation}
\usage{
auto_catboost_reg(
  x,
  y,
  cat_features = NULL,
  has_time = FALSE,
  fold_count = 10,
  type = "Classical",
  partition_random_seed = 0,
  shuffle = TRUE,
  stratified = FALSE,
  early_stopping_rounds = NULL,
  iterations = list(lower = 500, upper = 1000),
  learning_rate = list(lower = 0.001, upper = 0.05),
  l2_leaf_reg = list(lower = 0, upper = 5),
  depth = list(lower = 1, upper = 10),
  bagging_temperature = list(lower = 0, upper = 100),
  rsm = list(lower = 0, upper = 1),
  border_count = list(lower = 1, upper = 254),
  logging_level = "Silent",
  bo_iters = 10
)
}
\arguments{
\item{x}{Independent variables (features)}

\item{y}{Dependent variable}

\item{cat_features}{Name of the categorical variables}

\item{has_time}{Boolean, does data have time con}

\item{fold_count}{Number of cross-validation folds}

\item{type}{The type of cross-validation}

\item{partition_random_seed}{The random seed used for splitting pool into folds.}

\item{shuffle}{Shuffle the dataset objects before splitting into folds.}

\item{stratified}{Perform stratified sampling.}

\item{early_stopping_rounds}{Activates Iter over fitting detector with
od_wait set to early_stopping_rounds.}

\item{iterations}{The maximum number of trees that can be built when solving
machine learning problems.When using other parameters that limit the number
of iterations, the final number of trees may be less than the number
specified in this parameter. Default value: 1000}

\item{learning_rate}{The learning rate. Used for reducing the gradient step.
Default value: 0.03}

\item{l2_leaf_reg}{L2 regularization coefficient. Used for leaf value
calculation. Any positive values are allowed. Default value: 3}

\item{depth}{Depth of the tree. The value can be any integer up to 16.
It is recommended to use values in the range \code{[1; 10]}. Default value: 6}

\item{bagging_temperature}{Controls intensity of Bayesian bagging.
The higher the temperature the more aggressive bagging is.
Typical values are in the range \code{[0, 1]} (0 is for no bagging).
Possible values are in the range \code{0<= < +âˆž)}. Default value: 1}

\item{rsm}{Random subspace method.
The percentage of features to use at each iteration of building trees.
At each iteration, features are selected over again at random.
The value must be in the range \code{[0;1]}. Default value: 1}

\item{border_count}{Maximum number of borders used in target binarization
for categorical features that need it. If TargetBorderCount is
specified in 'simple_ctr', 'combinations_ctr' or 'per_feature_ctr' option
it overrides this value. Default value: 1}

\item{logging_level}{Possible values: 'Silent', 'Verbose', 'Info', 'Debug'
Default value: 'Silent'}

\item{bo_iters}{Maximum iteration for Bayesian optimazation.Default value: 10}
}
\description{
\Sexpr[results=rd, stage=render]{lifecycle::badge("experimental")}
Automatic model tuning using Bayesian optimization
}
\examples{
\dontrun{
# A toy example

data(iris, package = "datasets")

fit <- auto_catboost_reg(
  iris,
  label_col_name = "Petal.Length",
  cat_features = "Species",
  has_time = FALSE,
  fold_count = 3,
  type = "Classical",
  partition_random_seed = 0,
  shuffle = TRUE,
  stratified = FALSE,
  early_stopping_rounds = NULL,
  iterations = list(lower = 100, upper = 110),
  learning_rate = list(lower = 0.001, upper = 0.05),
  l2_leaf_reg = list(lower = 0, upper = 5),
  depth = list(lower = 1, upper = 10),
  bagging_temperature = list(lower = 0, upper = 100),
  rsm = list(lower = 0, upper = 1),
  border_count = list(lower = 1, upper = 254),
  bo_iters = 2
)


varimp <- get_var_imp(fit$model)

plot_varimp(varimp)
}
}
\author{
Resul Akay
}
